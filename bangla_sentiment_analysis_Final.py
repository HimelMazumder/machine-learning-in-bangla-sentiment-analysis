# -*- coding: utf-8 -*-
"""Bangla_Sentiment_Analysis (6).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cFt8M2aVe6C-WcgwZzgTt2viwAO6_xUj

# Setting up working directory
"""

from google.colab import drive
drive.mount('/content/drive', force_remount= True)

import os
root_directory = "/content/drive/My Drive/"
project_folder = "Colab Notebooks/Bangla_Sentiment_Analysis/"

# create and set working directory
def create_and_set_working_directory(project_folder):
	if os.path.isdir(root_directory + project_folder) == False:
		os.mkdir(root_directory + project_folder)
		print(root_directory + project_folder + "didn't exist and so created")
	
	# changing the OS path to project folder as working directory
	os.chdir(root_directory + project_folder)

create_and_set_working_directory(project_folder)

"""# Loading main dataset"""

import pandas as pd

dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/bangla_news_comments_12000.csv")

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/dataframe_main.csv'
with open(path, 'w', encoding = 'utf-8-sig') as f:
  dataframe.to_csv(f)

dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/dataframe_main.csv")
dataframe.head()

"""# Analyzing dataset"""

import numpy as np
import matplotlib.pyplot as plt

def draw_bar_graph(values, freq, color, x_label, y_label, title):
  fig = plt.figure(figsize = (10, 5))

  # creating the bar plot
  plt.bar(values, freq, color =color, width = 0.4)
 
  plt.xlabel(x_label)
  plt.ylabel(y_label)
  plt.title(title)
  plt.show()

def analyze_dataset(df, target):
  print(f'Shape: {df.shape}')
  print("")

  target_info = df[target].value_counts()
  print(f'Target class info:\n{target_info}')
  print("")

  data = target_info.to_dict()

  class_values = list(data.keys())
  class_value_freq = list(data.values())

  draw_bar_graph(class_values, class_value_freq, 'blue', 'sentiment type', 'frequency', 'target analysis')

analyze_dataset(dataframe, 'Tag')

"""**?? Looks a bit imbalanced. Balancing could be done based on accuracy scores**

# Dividing main dataset into smaller parts
"""

sub_dataframe = dataframe.iloc[0:118, :]
reserved_portion = dataframe.drop(dataframe.index[0:118])

for i in range(15):
  
  path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/sub_dataset_' + str(i) + '.csv'

  with open(path, 'w', encoding = 'utf-8-sig') as f:
    sub_dataframe.to_csv(f)

  print(f'part {i}: sub dataset size: {sub_dataframe.shape}')
  print(f'part {i}: remaining dataset size: {reserved_portion.shape}')
  print("")

  sub_dataframe = reserved_portion.iloc[0:118, :] 
  reserved_portion.drop(reserved_portion.index[0:118], inplace=True)
  

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/sub_dataset_15.csv'
with open(path, 'w', encoding = 'utf-8-sig') as f:
  sub_dataframe.to_csv(f)

"""# Basic preprocessing"""

pip install bltk

!pip install git+https://github.com/banglakit/bengali-stemmer.git

pip install emoji

!cp -r /content/drive/MyDrive/Colab\ Notebooks/Bangla_Sentiment_Analysis/bn_nlp /content

from emoji import demojize
import re
import string
from bltk.langtools.banglachars import (operators, punctuations, others, digits)

from bn_nlp.Stemmer import stemmerOP
from bengali_stemmer.rafikamal2014 import RafiStemmer

from bn_nlp.preprocessing import ban_processing

def process_text(txt):
  # converting emojis to text
  # words are separated by UNDERSCORE '_'
  txt = demojize(txt, delimiters=(" ", " "))

  # removing url from text
  url_regex = r"(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:'\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô]))"
  txt = re.sub(url_regex, '', txt)

  # removing english special characters expect '_'
  string.punctuation = string.punctuation.replace('_', '')
  txt = txt.translate(str.maketrans('', '', string.punctuation))

  # removing bengali special characters
  bangla_special_characters = '"‚Äò`'

  for x in operators:
    bangla_special_characters += x

  for x in punctuations:
    if x == '_':
      continue
    bangla_special_characters += x

  for x in others:
    if x == '‡ßç':
      continue
    bangla_special_characters += x

  # print(f"Bangla special characters: {bangla_special_characters}")
  # print("")

  txt = txt.translate(str.maketrans('', '', bangla_special_characters))

  # removing english numeric characters
  remove_digits = str.maketrans('', '', string.digits)
  txt = txt.translate(remove_digits)

  # removing bangla numeric characters
  bangla_digits = ""
  for x in digits:
      bangla_digits += x

  remove_bangla_digits = str.maketrans('', '', bangla_digits)
  txt = txt.translate(remove_bangla_digits)

  # changing the case of english words
  txt = txt.lower()

  # removing extra SPACES in the text
  txt = re.sub(' +', ' ', txt).strip()

  # stemming bangla words
  stemmer = RafiStemmer()
  txt = stemmer.stem_word(txt)
  
  stemmer = stemmerOP()
  txt = stemmer.stem(txt)

  # normalizing bangla words and removing bangla stopwords from the comment
  bp = ban_processing()
  
  # normalization: ‡¶Ö‡¶∏‡¶π‡¶®‡ßÄ‡ßü ‡¶≠‡¶æ‡¶∞‡ßÄ ‡¶¨‡¶∞‡ßç‡¶∑‡¶£‡ßá to ‡¶Ö‡¶∏‡¶π‡¶®‡¶ø‡ßü ‡¶≠‡¶æ‡¶∞‡¶ø ‡¶¨‡¶∞‡ßç‡¶∑‡¶®‡ßá
  txt = bp.word_normalize(txt)
  txt = bp.stop_word_remove(txt)

  return txt

text = '"‡¶è‡¶á ‡¶ñ‡¶¨‡¶∞ ‡¶è ‡¶Ü‡¶Æ‡¶æ‡¶ó‡ßã ‡¶ï‡¶ø? ü§¨ü§¨ https://machinelearningmastery.com/clean-text-machine-learning-python/"'
print(f'actual text: {text}')
text = process_text(text)
print(f'processed text: {text}')
print("")

text = "‡¶≤‡¶ø‡¶ñ‡¶æ‡¶∞ ‡¶∏‡¶Æ‡ßü ‡¶™‡¶æ‡¶∞‡¶≤‡ßá `‡¶∏‡¶§‡ßç‡¶Ø` yOu piece of !@##$ik ‡¶≤‡¶ø‡¶ñ‡¶æ‡¶∞ ‡¶Ö‡¶≠‡ßç‡¶Ø‡¶æ‡¶∏ ‡¶∂‡¶ø‡¶ñ‡ßÅ‡¶®‡•§"
print(f'actual text: {text}')
text = process_text(text)
print(f'processed text: {text}')
print("")

text = "‡¶è‡¶∞‡¶æ ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá‡¶á (YA'LL should go to HElL...), dogs! ‡¶Ø‡¶æ‡¶¨‡ßá ‡¶∏‡ßá‡¶ñ‡¶æ‡¶®‡ßá‡¶á ‡¶ö‡ßÅ‡¶∞‡¶ø ‡¶π‡¶¨‡ßá‡•§"
print(f'actual text: {text}')
text = process_text(text)
print(f'processed text: {text}')
print("")

text = "  ‡¶®‡ßú‡¶æ‡¶ö‡ßú‡¶æ ‡¶®‡¶æ‡¶ï‡¶ø   ‡¶°‡¶æ‡¶®‡¶¨‡¶æ‡¶Æ?203"
print(f'actual text: {text}')
text = process_text(text)
print(f'processed text: {text}')
print("")

text = "'‡ß®‡ß¶‡ß¶‡ß©/‡ß™/‡ß´‡¶è‡¶∞ ‡¶¶‡¶ø‡¶ï‡ßá ‡¶Æ‡¶ø‡¶®‡¶ø‡¶ï‡ßá‡¶ü ‡¶ï‡¶ø‡¶®‡¶§‡¶æ‡¶Æ ‡ß®‡ß¶/‡ß®‡ß® ‡¶ü‡¶æ‡¶ï‡¶æ‡ßü ‡¶§‡¶ñ‡¶® ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡ßá‡¶§‡¶® ‡¶õ‡¶ø‡¶≤ ‡ß≠‡ß¶‡ß¶‡ß¶‡¶ü‡¶æ‡¶ï‡¶æ‡•§' ‡¶è‡¶ñ‡¶® ‡¶™‡¶æ‡¶á ‡ß¨‡ß¶,‡ß¶‡ß¶‡ß¶‡¶ü‡¶æ‡¶ï‡¶æ ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶ö‡¶æ‡¶≤ ‡¶ï‡¶ø‡¶®‡¶ø ‡ß´‡ß¶/‡ß´‡ß´‡¶ü‡¶æ‡¶ï‡¶æ‡¶∞ ‡¶≠‡ßá‡¶§‡¶∞!"
print(f'actual text: {text}')
text = process_text(text)
print(f'processed text: {text}')
print("")

text = "‡¶Ø‡ßá ‡¶¶‡ßá‡¶∂‡ßá ‡ßß‡ß¨ ‡¶á  ‡¶°‡¶ø‡¶∏‡ßá‡¶Æ‡ßç‡¶¨‡¶∞ ‡¶§‡¶æ‡¶∞‡¶ø‡¶ñ‡ßá‡¶ì ‡¶°‡¶ø‡¶â‡¶ü‡¶ø ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡ßü , ‡¶∏‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶è‡¶Æ‡¶® ‡¶π‡¶ì‡ßü‡¶æ ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™ ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶®‡¶æ ‡•§"
print(f'actual text: {text}')
text = process_text(text)
print(f'processed text: {text}')
print("")

text = "‡¶∏‡ßú‡¶ï‡ßá‡¶∞ ‡¶≠‡ßã‡¶ó‡¶æ‡¶®‡ßç‡¶§‡¶ø‡¶§‡ßá    ‡¶™‡ßú‡ßá‡¶®  ‡¶®‡¶ó‡¶∞‡¶¨‡¶æ‡¶∏‡ßÄ"
print(f'actual text: {text}')
text = process_text(text)
print(f'processed text: {text}')
print("")

text = "‡¶Ö‡¶∏‡¶π‡¶®‡ßÄ‡ßü ‡¶≠‡¶æ‡¶∞‡ßÄ ‡¶¨‡¶∞‡ßç‡¶∑‡¶£‡ßá"
print(f'actual text: {text}')
text = process_text(text)
print(f'processed text: {text}')
print("")

text = "‡¶ï‡¶∞‡¶ø‡¶Æ ‡¶ï‡¶æ‡¶ú‡¶ü‡¶ø ‡¶ï‡¶∞‡¶õ‡ßá"
print(f'actual text: {text}')
text = process_text(text)
print(f'processed text: {text}')
print("")

text = "‡¶∏‡ßú‡¶ï‡ßá‡¶∞12A'--,.:B‡¶ï‡¶æ‡¶∞‡¶£‡ßá"
print(f'actual text: {text}')
text = process_text(text)
print(f'processed text: {text}')
print("")

def process_feature(df, feat):

  for index, row in df.iterrows():
    text = row[feat]
    processed_text = process_text(text)

    df.at[index, "Comment"] = processed_text

  return df

"""# Skip it - Generating processed sub datasets

00.
"""

sub_dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/sub_dataset_0.csv")
processed_dataframe = process_feature(sub_dataframe, 'Comment')

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/processed_dataset_0.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  processed_dataframe.to_csv(f)

"""01."""

sub_dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/sub_dataset_1.csv")
processed_dataframe = process_feature(sub_dataframe, 'Comment')

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/processed_dataset_1.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  processed_dataframe.to_csv(f)

"""02."""

sub_dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/sub_dataset_2.csv")
processed_dataframe = process_feature(sub_dataframe, 'Comment')

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/processed_dataset_2.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  processed_dataframe.to_csv(f)

"""03."""

sub_dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/sub_dataset_3.csv")
processed_dataframe = process_feature(sub_dataframe, 'Comment')

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/processed_dataset_3.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  processed_dataframe.to_csv(f)

"""04."""

sub_dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/sub_dataset_4.csv")
processed_dataframe = process_feature(sub_dataframe, 'Comment')

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/processed_dataset_4.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  processed_dataframe.to_csv(f)

"""05."""

sub_dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/sub_dataset_5.csv")
processed_dataframe = process_feature(sub_dataframe, 'Comment')

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/processed_dataset_5.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  processed_dataframe.to_csv(f)

"""06."""

sub_dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/sub_dataset_6.csv")
processed_dataframe = process_feature(sub_dataframe, 'Comment')

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/processed_dataset_6.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  processed_dataframe.to_csv(f)

"""07."""

sub_dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/sub_dataset_7.csv")
processed_dataframe = process_feature(sub_dataframe, 'Comment')

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/processed_dataset_7.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  processed_dataframe.to_csv(f)

"""08."""

sub_dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/sub_dataset_8.csv")
processed_dataframe = process_feature(sub_dataframe, 'Comment')

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/processed_dataset_8.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  processed_dataframe.to_csv(f)

"""09."""

sub_dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/sub_dataset_9.csv")
processed_dataframe = process_feature(sub_dataframe, 'Comment')

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/processed_dataset_9.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  processed_dataframe.to_csv(f)

"""10."""

sub_dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/sub_dataset_10.csv")
processed_dataframe = process_feature(sub_dataframe, 'Comment')

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/processed_dataset_10.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  processed_dataframe.to_csv(f)

"""11."""

sub_dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/sub_dataset_11.csv")
processed_dataframe = process_feature(sub_dataframe, 'Comment')

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/processed_dataset_11.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  processed_dataframe.to_csv(f)

"""12."""

sub_dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/sub_dataset_12.csv")
processed_dataframe = process_feature(sub_dataframe, 'Comment')

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/processed_dataset_12.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  processed_dataframe.to_csv(f)

"""13."""

sub_dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/sub_dataset_13.csv")
processed_dataframe = process_feature(sub_dataframe, 'Comment')

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/processed_dataset_13.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  processed_dataframe.to_csv(f)

"""14."""

sub_dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/sub_dataset_14.csv")
processed_dataframe = process_feature(sub_dataframe, 'Comment')

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/processed_dataset_14.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  processed_dataframe.to_csv(f)

"""15."""

sub_dataframe = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/sub_dataset_15.csv")
processed_dataframe = process_feature(sub_dataframe, 'Comment')

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/processed_dataset_15.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  processed_dataframe.to_csv(f)

"""# Start from here --> Merging all sub dataset into one main dataset"""

dataframes = []
for i in range(15):
  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/processed_dataset_' + str(i) + '.csv')
  dataframes.append(df)

dataframe_processed = pd.concat(dataframes)
dataframe_processed.drop(dataframe_processed.columns[[1, 2, 5]], axis = 1, inplace = True)
dataframe_processed.index = range(dataframe_processed.shape[0])

dataframe_processed.shape

dataframe_processed.head()

"""# Data cleaning"""

def clean_dataset(df):
  # Removing duplicated comments
  total_duplicate_comments = df.duplicated().sum()

  if total_duplicate_comments != 0:
      # Dropping duplicated entry keeping the first one. (default: keep='first')
      df.drop_duplicates(inplace=True)
  else:
      print("no duplicated entry found!")
      pass

  # Detecting and removing entries with missing values
  total_entries_with_missing_values = df.isnull().any(axis=1).sum()

  if total_entries_with_missing_values != 0:
      # Dropping the entries with at least missing values
      df.dropna(inplace=True)
  else:
      print("no entry with missing value found!")
      pass

  return df

dataframe_processed = clean_dataset(dataframe_processed)

"""# Target value encoding & decoding

Encode
"""

def encode_target_values(df, target):
  target_value_encode_dict = {
    "Very Positive": 2,
    "Positive": 1,
    "Neutral": 0,
    "Negative": -1,
    "Very Negative": -2
  }

  for index, row in df.iterrows():     
    sentiment = row[target]
    df.at[index, target] = target_value_encode_dict[sentiment]

  return df

dataframe_processed = encode_target_values(dataframe_processed, 'Tag')
dataframe_processed.head(10)

"""Decode"""

def decode_target_values(df, target):
  target_value_decode_dict = {
    2 : "Very Positive",
    1 : "Positive",
    0 : "Neutral",
    -1 : "Negative",
    -2 : "Very Negative"
  }
  target_value_decode_list = []

  for index, row in df.iterrows():     
    sentiment = row[target]
    df.at[index, target] = target_value_decode_dict[sentiment]

  return df

dataframe_processed = decode_target_values(dataframe_processed, 'Tag')
dataframe_processed.head(10)

"""store file to drive"""

y = len(dataframe_processed)

for i in range(y):
  dataframe_processed.iloc[i, 0] = i

path = '/content/drive/MyDrive/Colab Notebooks/Bangla_Sentiment_Analysis/dataframe_processed.csv'
with open(path, 'w', encoding = 'utf-8-sig') as f:
  dataframe_processed.to_csv(f)

"""# Analyze processed dataframe"""

analyze_dataset(dataframe_processed, 'Tag')

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

def draw_line_graph(f_lsts, f_labels, x_label, y_label, title, lgnd_txt):
  color_set = ['blue', 'green', 'red', 'black']
  color = []
  figure(figsize=(15, 7), dpi=80)

  # x = np.arange(x_val)
  for i in range(len(f_lsts)):
    plt.plot(np.array(f_lsts[i]), label=f_labels[i], marker='*')

  plt.xlabel(x_label)
  plt.ylabel(y_label)
  plt.title(title)

  plt.legend(lgnd_txt)
  plt.show()

text_len_list = []
processed_text_len_list = []

count = 0
l = dataframe_processed.shape[0]

for i in range(l):
  if 200 <= i <= 350:
    text  = dataframe.loc[i , 'Comment']
    text_len = len(text)
    text_len_list.append(text_len)

    processed_text = dataframe_processed.loc[i , 'Comment']
    processed_text_len = len(processed_text)
    processed_text_len_list.append(processed_text_len)

    if processed_text_len > text_len:
      print(count)
      print(f'raw: {text_len}')    
      print(f'processed: {processed_text_len}')  
    
      print("")

  count = count + 1

lsts = [text_len_list, processed_text_len_list]
labels = ['text size', 'processed text size']
legend = ['original text', 'processed text']

draw_line_graph(lsts, labels, 'instances', 'length', 'original vs processed text length', legend)

"""# Pattern Recognition process starts from here

Machine Learning based models
"""

df = dataframe_processed

feature = df['Comment']
target = df['Tag']

feature

target

from sklearn.model_selection import train_test_split

train, test = train_test_split(df, test_size=.2, random_state=50, stratify=df['Tag'])

train

test

train_x = train['Comment']
train_y = train['Tag']

train_x

train_y

test_x = test['Comment']
test_y = test['Tag']

test_x

test_y

"""# *TF - IDF vectorization process*"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

vect = TfidfVectorizer()
vc = CountVectorizer()

feature_dtm = vect.fit_transform(feature)
feature_cv = vc.fit_transform(feature)

vect.fit(train_x)

train_x_dtm = vect.transform(train_x)

train_x_dtm

train_x_dtm.shape

print(train_x_dtm)

test_x_dtm = vect.transform(test_x)

test_x_dtm

test_x_dtm.shape

print(test_x_dtm)

"""# Model based accuracy track"""

acc_lst = []
mdl_lst = []

"""# Naive Bayes Model implementation

Normal Naive Bayes
"""

from sklearn.naive_bayes import MultinomialNB

NB_classifier = MultinomialNB()
NB_classifier.fit(train_x_dtm, train_y)

NB_prediction = NB_classifier.predict(test_x_dtm)

from sklearn import metrics

acc_score = metrics.accuracy_score(test_y, NB_prediction)
acc_score

acc_lst.append(acc_score)
mdl_lst.append('NB')

metrics.confusion_matrix(test_y, NB_prediction)

"""Naive Bayes with K-Fold cross validation"""

from sklearn.model_selection import cross_val_score

k_values = [3, 5, 10]
scores = []

for k in k_values:
  score_set = cross_val_score(NB_classifier, feature_dtm, target, cv=k)
  scores.append(float(np.average(score_set)))

scores

from sklearn.model_selection import cross_val_score

score_set = cross_val_score(NB_classifier, feature_dtm, target, cv=3)
acc_score = np.average(score_set)
acc_score

acc_lst.append(acc_score)
mdl_lst.append('NB_KFold')

"""# Logistic Regression model implementation

Normal Logistic Regression
"""

from sklearn.linear_model import LogisticRegression

LR_classifier = LogisticRegression()

LR_classifier.fit(train_x_dtm, train_y)

LR_prediction = LR_classifier.predict(test_x_dtm)

acc_score = metrics.accuracy_score(test_y, LR_prediction)
acc_score

acc_lst.append(acc_score)
mdl_lst.append('LR')

metrics.confusion_matrix(test_y, LR_prediction)

"""Logistic Regression with Grid search and K Fold (Hyperparameter tuning)"""

LR_classifier_gs = LogisticRegression(max_iter=1000)

param = {
    'C': range(1, 10),
    'penalty': ['l1', 'l2', 'elasticnet'],
    'solver': ['lbfgs', 'liblinear', 'sag', 'saga']
}

from sklearn.model_selection import GridSearchCV

grid_search_cv = GridSearchCV(LR_classifier_gs, param, cv=3)

grid_search_cv.fit(train_x_dtm, train_y)

grid_search_cv.best_params_

acc_score = grid_search_cv.best_score_
acc_score

acc_lst.append(acc_score)
mdl_lst.append('LR_GS')

"""# Deep Learming Model - LSTM Implementation"""

from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Embedding, SpatialDropout1D
from keras.layers import LSTM
from keras.layers import Dense
from keras.callbacks import EarlyStopping
from keras.preprocessing.sequence import pad_sequences

# Applying DL based model ----------------------------------------------------------------------------------------------
# LSTM Model
# The maximum number of words
max_words = 50000
# Max number of words in each comment
max_comment_length = 300

EMBEDDING_DIM = 100

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(df['Comment'].values)

feature = tokenizer.texts_to_sequences(df['Comment'].values)
feature = pad_sequences(feature, maxlen=max_comment_length)

target = pd.get_dummies(df['Tag']).values

train_x_dl, test_x_dl, train_y_dl, test_y_dl = train_test_split(feature, target, test_size=.15, random_state=75)

train_x_dl.shape

model = Sequential()
model.add(Embedding(max_words, EMBEDDING_DIM, input_length=feature.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(13, activation='softmax'))
model.add(Dense(1, activation='softmax'))
model.add(Dense(5, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

epochs = 10
batch_size = 1500

history = model.fit(train_x_dl, train_y_dl, epochs=epochs, batch_size=batch_size, validation_split=0.1,
                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

accuracy = model.evaluate(test_x_dl, test_y_dl)
acc_score = accuracy[1]

print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accuracy[0], accuracy[1]))
print(acc_score)

acc_lst.append(acc_score)
mdl_lst.append('LSTM')

"""# Show accuracy graph for different models"""

draw_bar_graph(mdl_lst, acc_lst, 'red', 'model type', 'accuracy', 'Model accuracy analysis')